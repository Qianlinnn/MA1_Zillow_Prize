{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-df26666506b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "lgb_weight = 0.2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "##### READ IN RAW DATA\n",
    "path=''\n",
    "prop = pd.read_csv(path + '/input/properties_2016.csv')\n",
    "train = pd.read_csv(path + \"/input/train_2016_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS DATA FOR LIGHTGBM\n",
    "\n",
    "print( \"\\nProcessing input for LightGBM ...\" )\n",
    "for c, dtype in zip(prop.columns, prop.dtypes):\n",
    "    if dtype == np.float64:\n",
    "        prop[c] = prop[c].astype(np.float32)\n",
    "\n",
    "df_train = train.merge(prop, how='left', on='parcelid')\n",
    "\n",
    "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)\n",
    "y_train = df_train['logerror'].values\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "train_columns = x_train.columns\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "del df_train; gc.collect()\n",
    "\n",
    "split = 90000\n",
    "x_train, y_train, x_valid, y_valid = x_train[:split], y_train[:split], x_train[split:], y_train[split:]\n",
    "x_train = x_train.values.astype(np.float32, copy=False)\n",
    "x_valid = x_valid.values.astype(np.float32, copy=False)\n",
    "\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "d_valid = lgb.Dataset(x_valid, label=y_valid)\n",
    "\n",
    "\n",
    "\n",
    "##### RUN LIGHTGBM\n",
    "\n",
    "params = {}\n",
    "params['max_bin'] = 10\n",
    "params['learning_rate'] = 0.0021 # shrinkage_rate\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'l1'          # or 'mae'\n",
    "params['sub_feature'] = 0.5      # feature_fraction -- OK, back to .5, but maybe later increase this\n",
    "params['bagging_fraction'] = 0.85 # sub_row\n",
    "params['bagging_freq'] = 40\n",
    "params['num_leaves'] = 512        # num_leaf\n",
    "params['min_data'] = 500         # min_data_in_leaf\n",
    "params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n",
    "\n",
    "watchlist = [d_valid]\n",
    "print(\"\\nFitting LightGBM model ...\")\n",
    "clf = lgb.train(params, d_train, 430, watchlist)\n",
    "\n",
    "del d_train, d_valid; gc.collect()\n",
    "del x_train, x_valid; gc.collect()\n",
    "\n",
    "print(\"\\nPrepare for LightGBM prediction ...\")\n",
    "print(\"   Read sample file ...\")\n",
    "sample = pd.read_csv(path + '/input/sample_submission.csv')\n",
    "print(\"   ...\")\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "print(\"   Merge with property input ...\")\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')\n",
    "print(\"   ...\")\n",
    "del sample, prop; gc.collect()\n",
    "print(\"   ...\")\n",
    "x_test = df_test[train_columns]\n",
    "print(\"   ...\")\n",
    "del df_test; gc.collect()\n",
    "print(\"   Preparing x_test...\")\n",
    "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n",
    "    x_test[c] = (x_test[c] == True)\n",
    "print(\"   ...\")\n",
    "x_test = x_test.values.astype(np.float32, copy=False)\n",
    "\n",
    "print(\"\\nStart LightGBM prediction ...\")\n",
    "# num_threads > 1 will predict very slow in kernal\n",
    "clf.reset_parameter({\"num_threads\":1})\n",
    "p_test = clf.predict(x_test)\n",
    "\n",
    "p_test = 0.97*p_test + 0.03*0.011\n",
    "\n",
    "del x_test; gc.collect()\n",
    "\n",
    "print( \"\\nAdjusted LightGBM predictions:\" )\n",
    "print( pd.DataFrame(p_test).head() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RE-READ PROPERTIES FILE\n",
    "\n",
    "\n",
    "print( \"\\nRe-reading properties file ...\")\n",
    "properties = pd.read_csv(path + '/input/properties_2016.csv')\n",
    "\n",
    "\n",
    "\n",
    "##### PROCESS DATA FOR XGBOOST\n",
    "\n",
    "print( \"\\nProcessing input for XGBoost ...\")\n",
    "for c in properties.columns:\n",
    "    properties[c]=properties[c].fillna(-1)\n",
    "    if properties[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(properties[c].values))\n",
    "        properties[c] = lbl.transform(list(properties[c].values))\n",
    "\n",
    "train_df = train.merge(properties, how='left', on='parcelid')\n",
    "x_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n",
    "x_test = properties.drop(['parcelid'], axis=1)\n",
    "# shape\n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "# drop out ouliers\n",
    "train_df=train_df[ train_df.logerror > -0.4 ]\n",
    "train_df=train_df[ train_df.logerror < 0.418 ]\n",
    "x_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n",
    "y_train = train_df[\"logerror\"].values.astype(np.float32)\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "print('After removing outliers:')\n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "\n",
    "\n",
    "##### RUN XGBOOST\n",
    "\n",
    "print(\"\\nSetting up input for XGBoost ...\")\n",
    "# xgboost params\n",
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,\n",
    "    'alpha': 0.4,\n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "# Enough with the ridiculously overfit parameters.\n",
    "# I'm going back to my version 20 instead of copying Jayaraman.\n",
    "# I want a num_boost_rounds that's chosen by my CV,\n",
    "# not one that's chosen by overfitting the public leaderboard.\n",
    "# (There may be underlying differences between the train and test input\n",
    "#  that will affect some parameters, but they shouldn't affect that.)\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "# cross-validation\n",
    "#print( \"Running XGBoost CV ...\" )\n",
    "#cv_result = xgb.cv(xgb_params,\n",
    "#                   dtrain,\n",
    "#                   nfold=5,\n",
    "#                   num_boost_round=350,\n",
    "#                   early_stopping_rounds=50,\n",
    "#                   verbose_eval=10,\n",
    "#                   show_stdv=False\n",
    "#                  )\n",
    "#num_boost_rounds = len(cv_result)\n",
    "\n",
    "# num_boost_rounds = 150\n",
    "num_boost_rounds = 242\n",
    "print(\"\\nXGBoost tuned with CV in:\")\n",
    "print(\"   https://www.kaggle.com/aharless/xgboost-without-outliers-tweak \")\n",
    "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n",
    "\n",
    "# train model\n",
    "print( \"\\nTraining XGBoost ...\")\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "print( \"\\nPredicting with XGBoost ...\")\n",
    "xgb_pred = model.predict(dtest)\n",
    "\n",
    "print( \"\\nXGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred).head() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### COMBINE PREDICTIONS\n",
    "\n",
    "print( \"\\nCombining XGBoost and LightGBM predicitons ...\" )\n",
    "xgb_weight = 1 - lgb_weight\n",
    "pred = xgb_weight*xgb_pred + lgb_weight*p_test\n",
    "\n",
    "print( \"\\nCombined predictions:\" )\n",
    "print( pd.DataFrame(pred).head() )\n",
    "\n",
    "\n",
    "\n",
    "##### WRITE THE RESULTS\n",
    "\n",
    "print( \"\\nPreparing results for write ...\" )\n",
    "y_pred=[]\n",
    "\n",
    "for i,predict in enumerate(pred):\n",
    "    y_pred.append(str(round(predict,4)))\n",
    "y_pred=np.array(y_pred)\n",
    "\n",
    "output = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n",
    "        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n",
    "        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n",
    "# set col 'ParceID' to first col\n",
    "cols = output.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "output = output[cols]\n",
    "from datetime import datetime\n",
    "\n",
    "print( \"\\nWriting results to disk ...\" )\n",
    "output.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n",
    "\n",
    "print( \"\\nFinished ...\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
