{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "# from sklearn.preprocessing import Imputer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#########Loading the Files################\n",
    "\n",
    "props = pd.read_csv('input/properties_2016.csv')\n",
    "train_df = pd.read_csv(\"input/train_2016_v2.csv\")\n",
    "test_df = pd.read_csv(\"input/sample_submission.csv\")\n",
    "test_df = test_df.rename(columns={'ParcelId': 'parcelid'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with Merged Operation\n",
      "We have 57 features.\n"
     ]
    }
   ],
   "source": [
    "######Merge Operation#####\n",
    "\n",
    "train = train_df.merge(props, how = 'left', on = 'parcelid')\n",
    "test = test_df.merge(props, on='parcelid', how='left')\n",
    "\n",
    "for c in train.columns:\n",
    "    if train[c].dtype == 'float64':\n",
    "        train[c] = train[c].values.astype('float32')\n",
    "        \n",
    "\n",
    "print(\"Done with Merged Operation\")        \n",
    "#####Removing Outliers, Total Features#####        \n",
    "train=train[ train.logerror > -0.4 ]\n",
    "train=train[ train.logerror < 0.4 ]\n",
    "\n",
    "\n",
    "\n",
    "do_not_include = ['parcelid', 'logerror', 'transactiondate']\n",
    "\n",
    "feature_names = [f for f in train.columns if f not in do_not_include]\n",
    "\n",
    "print(\"We have %i features.\"% len(feature_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the Encoding\n"
     ]
    }
   ],
   "source": [
    "#####Getting the same number of columns for Train, Test######\n",
    "\n",
    "y = train['logerror'].values\n",
    "\n",
    "train = train[feature_names].copy()\n",
    "test = test[feature_names].copy()\n",
    "\n",
    "#####Handling Missing Values#####     \n",
    "\n",
    "for i in range(len(train.columns)):\n",
    "    train.iloc[:,i] = (train.iloc[:,i]).fillna(-1)\n",
    "\n",
    "for i in range(len(test.columns)):\n",
    "    test.iloc[:,i] = (test.iloc[:,i]).fillna(-1)    \n",
    "    \n",
    "#####Encoding the Categorical Variables#####\n",
    "\n",
    "lbl = LabelEncoder()\n",
    "\n",
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl.fit(list(train[c].values))\n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "\n",
    "for c in test.columns:\n",
    "    if test[c].dtype == 'object':\n",
    "        lbl.fit(list(test[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))     \n",
    "        \n",
    "print(\"Done with the Encoding\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Normalizing the values####\n",
    "\n",
    "mmScale = MinMaxScaler()\n",
    "\n",
    "n = train.shape[1]\n",
    "\n",
    "\n",
    "x_train = mmScale.fit_transform(train)\n",
    "x_test = mmScale.fit_transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Neural Network\n",
      "Epoch 1/5\n",
      "88431/88431 [==============================] - 28s 316us/step - loss: 0.0069 - mae: 0.0536\n",
      "Epoch 2/5\n",
      "88431/88431 [==============================] - 26s 295us/step - loss: 0.0068 - mae: 0.0530\n",
      "Epoch 3/5\n",
      "88431/88431 [==============================] - 28s 316us/step - loss: 0.0068 - mae: 0.0529\n",
      "Epoch 4/5\n",
      "88431/88431 [==============================] - 28s 316us/step - loss: 0.0068 - mae: 0.0529\n",
      "Epoch 5/5\n",
      "88431/88431 [==============================] - 29s 327us/step - loss: 0.0068 - mae: 0.0529\n"
     ]
    }
   ],
   "source": [
    "#####Artificial Neural Networks Implementation#####\n",
    "print(\"Starting Neural Network\")\n",
    "\n",
    "model_n = Sequential()\n",
    "\n",
    "#Want to use an expotential linear unit instead of the usual relu\n",
    "model_n.add( Dense( n, activation='relu', input_shape=(n,) ) )\n",
    "model_n.add( Dense( int(0.5*n), activation='relu' ) )\n",
    "model_n.add(Dense(1, activation='linear'))\n",
    "model_n.compile(loss='mse', optimizer='rmsprop', metrics=['mae'])\n",
    "        \n",
    "\n",
    "model_n.fit(x_train, y, epochs=5, batch_size=10)\n",
    "\n",
    "predict_test_NN = model_n.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing csv ...\n"
     ]
    }
   ],
   "source": [
    "#####Writing the Results######\n",
    "\n",
    "sub = pd.read_csv('input/sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = predict_test_NN\n",
    "\n",
    "print('Writing csv ...')\n",
    "sub.to_csv('NN_submission.csv', index=False, float_format='%.4f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
